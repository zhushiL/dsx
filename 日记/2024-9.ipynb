{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9/2 周一\n",
    "全新的一周，开始新篇章！！！\n",
    "\n",
    "**首先应对的是 关于LLM llama**\n",
    " - 模型架构没有那么难，所以不要害怕\n",
    " - 主要花大量时间精力去准备数据\n",
    " - scaling law 预测大模型\n",
    " - 机器集群训练大模型的挑战性\n",
    "\n",
    "当让也学习到一些新东西；首先是回忆算 $attention$ 的时候是下一个 Q 去乘 所有的 K、V ，然后使用 $GQV$ 可以降低存储\n",
    "\n",
    "--------\n",
    "接下来的工作指导：\n",
    "1. 希望接下来能研究一下 LLM 相关的工作，能做下去\n",
    "2. 看一看后续的论文精读，能不能继续找到一些灵感\n",
    "3. 可以思考 $CLIP$ 文本结合的 $DETR$ 目标检测任务，可以在短时间内做出成果\n",
    "\n",
    "我是`有时间试错`的，不要急！！！\n",
    "可恶，一写到这就有点着急了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9/3 周二\n",
    "现在是晚上十点，总结时间。所以今天干嘛了？     \n",
    "Neural Corpus Indexer 这篇文章，看了LM讲解，自己看了看论文。 发在nips上的文章      \n",
    "发现对文档检索这个方向有点了解，多看一点没问题，技术迁移嘛。三个月做一个[**ideal**]没问题，做就行了。 just do it\n",
    "      \n",
    "自从自己投过文章后，现在会更注重论文写作这一块，确实很有技巧，好的写作胜过技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9/4 周四\n",
    "论文由于没有引用期刊论文被退回，昨天到今天基本忙于修改论文。      \n",
    "但是我真的感觉效率好低，往这一坐就不想干（哭）。   \n",
    "\n",
    "差不多看完了 NCI 这篇文章，感觉写的确实有技巧，在有限的篇幅里把自己做的事情交代清楚。\n",
    "- 同时，注意到把自己的训练设置说的很清楚，评测标准也表达清楚，这很难得。   \n",
    "- 但是，实验也是简单陈述，分析就是一本正经胡扯了。\n",
    "\n",
    "就是发现自己 transform 还是有所欠缺，还有 seq2seq 模型，再了解一下吧，搞熟一点，下次就可以乱编公式了，哈哈哈哈哈！！！     \n",
    "比较自己的论文就是缺一点公式为自己的论文增添一点玄幻色彩。  \n",
    "\n",
    "--------\n",
    "又开始回忆过去，我的脑袋有点乱，本来充满希望的。怎么回事啊，一直缠着走不开了。   \n",
    "谈一次恋爱，痛苦这么久，真的令人伤心。   \n",
    "你说我明天能不能调整好状态，我肯定说不能，又开始循环了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9/9 周一\n",
    "不知不觉，怎么就到9号了呀，时间过的好快。   \n",
    "6/7/8在生病 哭   \n",
    "总结今天把审稿意见写了，保底这篇文章能中了，运气挺好的！！！！！  \n",
    "   \n",
    "#### 总结上周干了啥：  \n",
    "主要就是阅读语言模型类文章，然后找思路，有一定收获，但是需要总结。\n",
    "明天我将继续语言类模型研究，这周必须拿下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9/10 周二\n",
    "哎呀，头疼，感觉病越来越严重了。但是，但是，还是要寻找一下研究思路。     \n",
    "资源算力不够，期望能寻找到不消耗算力，简单的方向：\n",
    "- 利用好预训练好的模型，做$fine-tune$ **(vit; clip)**\n",
    "- 数据增强，一些即插即用的模块\n",
    "- 定义新的任务，目标函数，topic，新的损失函数\n",
    "- 多模态、强化学习、对比学习、无监督都拿来用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9/12 周四\n",
    "\n",
    "![NCI总览图](./image/NCI.jpg)\n",
    "\n",
    "#### 1. A Neural Corpus Indexer for Document Retrieval (NCI)\n",
    "- 使用[**K-means**]聚类算法将文档映射到 *docid*。\n",
    "\n",
    "- 用一个预训练好的模型从文档中抽取 *query* ，这样 *query* 由两部分组成。\n",
    "    - Generated Query；\n",
    "    - Ground-Truth Query；\n",
    "    \n",
    "- 最后使用一个标准的 *seq2seq* 模型将 *query* 作为输入预测 *docid*。\n",
    "    - 设计 PAWA Decoder 层级结构预测标号；\n",
    "    - 损失由分类损失和两种 *query* 的对比损失组成；\n",
    "\n",
    "![InstructGPT结构](./image/InstructGPT.jpg)\n",
    "\n",
    "#### 2. Training language models to follow instructions with human feedback (InstructGPT)\n",
    "- 人工收集和API收集一个高质量的数据集有监督的微调 **GPT3**，训练一个 *SFT*(supervised fine-tuning) 模型。\n",
    "\n",
    "- 以 *SFT* 模型为基础根据模型输出结果排序训练一个奖励模型 *RM*(reward model)，使得模型输出结果更符合人的偏好。\n",
    "    - 人工对9个输出结果排序\n",
    "    - 使用排序损失 **pair-wise ranking loss**   \n",
    "![pair-wise ranking loss](./image/pairwise_ranking_loss.jpg)\n",
    "\n",
    "- 训练最终的强化学习模型 *RL*(reinforement learning)，使用强化学习策略 **PPO**。      \n",
    "    - 第一项由 RM 提供标签计算损失；\n",
    "    - 第二项 KL 散度，保证模型与原始 SFT 模型不要偏离太远；\n",
    "    - 第三项引入原始的预训练损失，维持模型语言任务上的能力；   \n",
    "![PPO损失函数](./image/PPO.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9/13 周五\n",
    "\n",
    "**COT**(china of thought)   \n",
    "讲述了一种一种简单的提升模型输出精度以及推理能力的技巧：引导模型在回答问题时写出推理过程。   \n",
    " \n",
    "对大模型的输入统称为 *prompt*，模型的输出结果对 *prompt* 敏感。   \n",
    "对大模型的输出方式有所了解(promt): \n",
    "- zero-shot: 写出任务，接上prompt描述；\n",
    "\n",
    "- few-shot: 写出任务，给模型举几个例子，紧接着附上想要模型回答的问题。\n",
    "\n",
    "------------\n",
    "然后我发现一件事情，authropic是从openAI分裂出来的，他们的模型叫 Claude，他们技术其实是和 chatgpt 很像的。好像就很八卦，好像可以继续看一下后续的故事，就是chatgpt4出来后发生的故事。好像随着了解，发现他们的技术有点越来越难，但是之前也说了，还是很基础的 transformer 的技术，强化学习对其还是 RLHF 那套，有什么特别强的事情吗。\n",
    "\n",
    "**whisper**可以玩一下，可以用到剪辑里面。\n",
    "   \n",
    "先了解一下这几个发展，我要开始继续看看多模态或者 cv 那边的事情了。有一篇开放词汇的分割，2023的 cvpr 看一下具体怎么做的。先看论文，代码复现一下也可。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9/14 周六\n",
    "GPT4的实验报告->大语言模型涌现的能力是无法想象的，但是由于transformer这种自回归以及无法解释性还是有一部分人持悲观态度。但是使用数据和算力探究其极限也不是我们普通人能触及的。不过，不过，还是要在这里留一个坑[**自己去微调部署一个语言模型**]   \n",
    "\n",
    "突然想到，可以用chatgpt去帮我读那些我想读但是看不下去的论文，多看一点论文没坏处的！！！   \n",
    "\n",
    "**Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models**(ODISE)      \n",
    "CVPR 2023 cite:302   \n",
    "使用文本到图像的扩散模型加上判别模型做开放词汇的全景分割，想法非常的好，还可以发掘一下文章里的相关工作看一下开放词汇的分割任务都是怎么做的。->直接chatgpt阅读，直接对上了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9/18 周三\n",
    "中秋节放假三天结束，刚回到工作岗位，但是我现在是斗志全无。我要干什么啊，不知道做什么，什么都做不下去。我要回顾一下我之前干了什么，要按照之前的计划接着往下做，要有目标，不是漫无目的的乱走。我已经虚度太长的时间了，我要坚持，但是我眼睛怎么这么花，我看不清。真的看不清。   \n",
    "好神奇，刚滴了眼药水，我能看清了，真的有物理阻隔，如果我眼睛都看不清我怎么看清这个世界，要调理好自己的身体，然后背起行囊坚定往前走。    \n",
    "看了一下四天前的工作，当务之急，还是先把　ODISE　这篇论文看了，，，思路怪。\n",
    "\n",
    "-----------\n",
    "\n",
    "![ODISE](./image/ODISE.jpg)\n",
    "\n",
    "#### Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models(ODISE)            \n",
    "本文使用文本到图像的扩散模型加判别模型，实现开放词汇的分割。\n",
    "\n",
    "- 使用 **CLIP** 的图像编码器接一个可学习的 MLP 得到隐式的文本嵌入，与其和图像共同输入预训练好的 **text-image Diffusion UNet** 得到图片的视觉表征。\n",
    "\n",
    "- 随后将视觉表征输入 Mask Generator(文中使用*Mask2Former*) 得到 mask embedding特征和掩码预测，得到的掩码预测会和已有的标签算 binary mask loss，学习生成合理的分割掩码。\n",
    "\n",
    "- 最后将掩码与开放词汇匹配，文中使用[**类别标签/图片描述**]两种方式训练。（知道所有标签则知道类别总数，因此使用交叉熵损失；图片描述中提取的名词数不确定，只能对比学习匹配最相似的内容）\n",
    "    - 在有训练图片标签集的时候使用 CLIP 文本编码器得到类别的 text embedding，使其与掩码生成器得到的特征算点积随后交叉熵计算损失。\n",
    "    - 若使用图片描述则挑出描述中名词编码计算点积后使用对比学习损失。\n",
    "\n",
    "/// 分割任务可分为两部分：1、分割掩码；2、给掩码分配对应的标签。   \n",
    "--->>> 很明显，途中的两处 loss 对应这两项指标。 mask loss 确保生成准确的分割的掩码，后续与文本标签算相似度，找出与 mask 对应的标签。\n",
    "     \n",
    "!!! 把预训练模型利用到极致，三个预训练好的模型，可学习参数量非常非常少。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9/20 周五 [**GPT三部曲**]\n",
    "\n",
    "#### 1. Improving Language Understanding by Generative Pre-Training\n",
    "使用 **transformer decoder** 做语言模型的生成任务预测未来。训练采用 预训练预测+下游任务微调 的策略。   \n",
    "**novelty：** 把预训练模型使用到 *NLP* 中，解决下游任务。   \n",
    "\n",
    "#### 2. Language Models are Unsupervised Multitask Learners\n",
    "使用更大的数据和模型训练更大的 1.5B 参数的 **GPT-2**。创造 prompt 训练方式，增强上下文理解，但是步子迈的太大，zero-shot的效果并不理想。   \n",
    "**novelty：** 提出 zero-shot的概念，新意度拉满。\n",
    "\n",
    "#### 3. Language Models are Few-Shot Learners\n",
    "使用更多的数据，更多的时间，训练参数更大的 175B 的 **GPT-3**。规范了 fine-tune, zero-shot, few-shot 的概念，大力出奇迹！同时也讨论了一下模型的安全和偏见问题。   \n",
    "- fine-tune: 在特定的数据集上微调模型，进行梯度更新。\n",
    "- few-shot: 在模型输出时提供几个样例，根据上下文理解有更好的性能。模型梯度不更新。\n",
    "- zero-shot: 不给模型提供样例，给出任务描述和 prompt 直接输出，梯度同样不更新。 \n",
    "\n",
    "Google 与 OpenAI 之间 Bert 和 GPT 之间的斗争引导我们研究不要一条路走到黑，有时候换个思路新意度一下就上来了！！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9/22 周日\n",
    "记录一下今天的碎碎念吧。      \n",
    "     \n",
    "虽然这么长时间过去了，但是感觉心里一直藏着一块石头，就隐隐难受，导致最近吧，反正就是很膈应，不能太好的集中注意力。但是但是对于这方面，我还是想了一下，我应该是持续的去散发魅力吸引别人，而不是去寻找别人，我要让别人主动起来，我的处境本来就不好，我应该让自己出彩，我要让生活更丰富，让自己更自信。互联网经营起来，提升自己魅力的高光时刻，这也是要花点心思在里面的。爱谁都是爱啊，别人主动对我好，那还要啥自行车啊。\n",
    "\n",
    "另外对于科研方面，我必须要好好的说一下了，自从回到学校一个月了，目前来说，我一直在吸收能量，广泛的看论文了解形式。沉淀！！！！我感觉我已经能摸到一点门道了，感觉灵感要迸发出来了，你知道吗，就像那种就差临门一脚就能突破的感觉，这种感觉很爽。准备再过一周我沉淀结束，我要正式开始工作。 今天也是有一些发现，对于 SAM 这篇论文的了解，我发现 NLP 和 CV 这边大一统的消息了，更强的预训练模型应用诞生了，这给我的科研方向给了更大的动力。最后就是发现 RT-DETR 被 CVPR 2024 接收了，发现 CVPR　不过如此，我要备战 CVPR 2025（突然发现投稿时间在十一月，根本来不及啊） ，对自己在科研上保持信心。看了那么多论文，在写作技巧上应该有更多的领悟。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9/23 周一\n",
    "希望一天时间可以把 SAM 拿下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "论文阅读预定    \n",
    "Sam GLIP BLIP DEIT  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
